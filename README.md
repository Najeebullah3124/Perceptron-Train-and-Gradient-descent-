# üß† Perceptron Learning Rule and Gradient Descent Delta Rule

In this project, we implement two machine learning algorithms: **Perceptron Learning Rule** and **Gradient Descent Delta Rule**. The goal is to train a model using these algorithms and evaluate their performance on the **Iris dataset**.

## üìã Problem Description

### Task 1: Perceptron Learning Rule and Gradient Descent Delta Rule Implementation
- **Perceptron** is a simple binary classification algorithm.
- **Gradient Descent** is used to minimize errors through iterative optimization.

### Task 2: Load the Iris Dataset
- We load the Iris dataset which contains 150 samples with 4 features for classification.

### Task 3: Split the Data into Training and Testing Sets
- The dataset is split into **80% training** and **20% testing** data.

### Task 4: Train the Models
- Both the **Perceptron** and **Gradient Descent** models are trained on the training data.

### Task 5: Evaluate Models on Test Set
- The models' accuracies are calculated on the test set.

### Task 6: Plot the Errors Over Epochs
- We plot the number of misclassifications over the epochs for both models to visually compare their learning performance.

## üß© Libraries Used
- **NumPy**: For numerical operations and matrix manipulation.
- **Scikit-learn**: For loading the Iris dataset and splitting it into training/testing sets.
- **Matplotlib**: For visualizing the errors over epochs.

## üèÜ Project Deliverables
1. **Perceptron Model**: Implemented with weight updates based on misclassifications.
2. **Gradient Descent Model**: Optimized using the Delta rule and sigmoid activation function.
3. **Evaluation**: Accuracy of both models is evaluated on a test set.
4. **Visualization**: A plot of the errors over epochs for comparison.
